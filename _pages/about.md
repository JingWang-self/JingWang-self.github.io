---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "<https://cdn.jsdelivr.net/gh/>" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "<https://raw.githubusercontent.com/>" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a first-year Ph.D. student in [Wangxuan Institute of Computer Technology](https://www.icst.pku.edu.cn/index.htm), [Peking University](https://www.pku.edu.cn/), supervised by [Prof. Huishuai Zhang](https://huishuai-git.github.io/) and [Prof. Dongyan Zhao](https://www.icst.pku.edu.cn/zhaodongyan/). I received a bachelor's degree in Computer Science from the [Chongqing University](https://www.cqu.edu.cn/). During my undergraduate studies, I had the privilege of working with [Prof. Fuqiang Gu](https://faculty.cqu.edu.cn/gufq/zh_CN/index.htm) on the topic of robotics/SLAM.

**<font color='red'>My research interest includes multimodal learning, vision-language models and video understanding.</font>**

# üî• News
- *2025.06*: &nbsp;üéâüéâ One paper (SAMPLE: Semantic Alignment through Temporal-Adaptive Multimodal Prompt Learning for Event-Based Open-Vocabulary Action Recognition) has been accepted by *ICCV-2025* as first author!

# üìù Publications

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/SAMPLE.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**<span style="color: #224b8d; font-weight: bold;">Semantic Alignment through Temporal-Adaptive Multimodal Prompt Learning for Event-Based Open-Vocabulary Action Recognition</span>**

**<font color='red'>Jing Wang</font>**, Rui Zhao, Ruiqin Xiong, Xingtao Wang, Xiaopeng Fan, and Tiejun Huang
**ICCV 2025**

[**PDF Coming Soon**](https://jingwang-self.github.io/) 

- Our work, SAMPLE, enables large vision-language models like CLIP to perform event-based open-vocabulary action recognition. By pioneering a Temporal-Adaptive Multimodal Prompt Learning strategy, it efficiently bridges the domain gap from static images to dynamic event streams while preserving generalization for unseen actions. This establishes a new state-of-the-art across all tested scenarios, from fully-supervised to zero-shot settings.

</div>
</div>



# üéñ Selected Honors and Awards

- *2024.05* Outstanding Undergraduate Graduate of Chongqing City.
- *2023.05* Advanced Individual in Science and Technology of Chongqing City.
- *2022.12* The National Second Prize **(Top 2.4%)** in the 2022 National College Student Mathematical Contest in Modeling (CUMCM), the Chinese Association of Mathematics and Applied Mathematics.
- *2022.05* Finalist Winner **(Top 2.5%)** in the 2022 Mathmatical Contest in Modeling (MCM/ICM), the COMAP of America.
- *2022.09* The National Scholarship for Bachelor Students.
- *2021.09* The National Scholarship for Bachelor Students.

# üìñ Educations

- *2024.09 - 2029.04 (expected)*, Peking University, Computer Science and Technology, Ph.D.
- *2020.09 - 2024.06*, Chongqing University, Computer Science and Technology, Bachelor **(Rank: 1/218)**.

# üíª Internships

- *2023.12 - 2024.03*, Research Intern - Applied Large Language Models, Shanghai AI Lab, China.
